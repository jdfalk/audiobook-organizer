#!/usr/bin/env python3
# file: scripts/test-organize-import-v3.py
# version: 1.4.0
# guid: 1a2b3c4d-5e6f-7a8b-9c0d-1e2f3a4b5c6d


"""
Test script to validate audiobook organizer with REAL metadata extraction.

Version 3 improvements:
- Reads embedded tags from audio files using ffprobe
- Prioritizes actual author/title from file metadata over directory names
- Falls back to path parsing only when tags are missing
- More accurate book identification and duplicate detection
"""

import argparse
import hashlib
import json
import os
import re
import subprocess
import sys
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path


@dataclass
class FileInfo:
    """Represents a single file that is part of an audiobook."""

    original_path: str
    filename: str
    extension: str
    is_chapter: bool = False
    chapter_number: int | None = None


@dataclass
class BookVersion:
    """Represents a specific version of an audiobook."""

    version_id: str
    files: list[FileInfo] = field(default_factory=list)
    primary_file: str | None = None
    format: str = ""
    total_files: int = 0
    is_multi_file: bool = False
    base_directory: str = ""


@dataclass
class BookMetadata:
    """Represents an audiobook with all its versions and files."""

    book_id: str
    title: str = ""
    author: str = ""
    series: str = ""
    position: int = 0
    narrator: str = ""
    album: str = ""
    confidence: str = "low"
    extraction_method: str = ""
    proposed_path: str = ""
    issues: list[str] = field(default_factory=list)
    versions: list[BookVersion] = field(default_factory=list)
    duplicate_group_id: str | None = None
    total_versions: int = 0
    metadata_source: str = ""  # "tags", "path", or "mixed"


class MetadataExtractor:
    """Extract metadata from audio files using ffprobe."""

    @staticmethod
    def extract_from_file(file_path: str) -> dict[str, str]:
        """
        Extract metadata from audio file using ffprobe.

        Returns dict with keys: title, artist, album, album_artist, genre, date, comment
        """
        metadata = {}

        try:
            # Run ffprobe to get metadata
            cmd = [
                "ffprobe",
                "-v",
                "quiet",
                "-print_format",
                "json",
                "-show_format",
                file_path,
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=5)

            if result.returncode == 0:
                data = json.loads(result.stdout)
                if "format" in data and "tags" in data["format"]:
                    tags = data["format"]["tags"]

                    # Normalize tag keys (ffprobe returns lowercase)
                    # Map common tag names
                    tag_mapping = {
                        "title": "title",
                        "artist": "artist",
                        "album": "album",
                        "album_artist": "album_artist",
                        "genre": "genre",
                        "date": "date",
                        "comment": "comment",
                        "composer": "composer",
                        "narrator": "narrator",
                        "publisher": "publisher",
                        "description": "description",
                    }

                    for tag_key, target_key in tag_mapping.items():
                        if tag_key in tags:
                            metadata[target_key] = tags[tag_key]

                    # Handle case variations
                    for key in tags:
                        lower_key = key.lower()
                        if lower_key == "album artist" or lower_key == "albumartist":
                            metadata["album_artist"] = tags[key]
                        elif "narrator" in lower_key and "narrator" not in metadata:
                            metadata["narrator"] = tags[key]
                        elif "author" in lower_key and "artist" not in metadata:
                            metadata["artist"] = tags[key]

        except (
            subprocess.TimeoutExpired,
            subprocess.SubprocessError,
            json.JSONDecodeError,
        ):
            pass

        return metadata


class SeriesPatternMatcher:
    """Pattern matching for series identification."""

    PATTERNS = [
        # "Series Name 03 Book Title" or "Series Name 03" - captures series name and number
        (re.compile(r"(?i)^(.+?)\s+(\d{2})\s+(.+)$"), "series_number_title"),
        # "Series Name, Book N" or "Series Name, Book N - Title"
        (
            re.compile(r"(?i)^(.+?),\s+Book\s+(\d+)(?:\s*-\s*(.+))?$"),
            "series_comma_book",
        ),
        # "Series Name Book N" or "Series Name Book N - Title"
        (
            re.compile(r"(?i)^(.+?)\s+Book\s+(\d+)(?:\s*-\s*(.+))?$"),
            "series_book_numbered",
        ),
        # "Series Name #N" or "Series Name #N - Title"
        (re.compile(r"(?i)^(.+?)\s+#(\d+)(?:\s*-\s*(.+))?$"), "series_hash_numbered"),
        # "Series Name Vol N" or "Series Name Volume N"
        (
            re.compile(r"(?i)^(.+?)\s+Vol(?:\.|ume)?\s+(\d+)(?:\s*-\s*(.+))?$"),
            "series_volume_numbered",
        ),
        # Generic "Series N - Title" (but not if it looks like a chapter)
        (
            re.compile(r"(?i)^(.+?)\s+(\d+)(?:\s*:|\s+-)\s+(.+)$"),
            "series_numbered_title",
        ),
    ]

    SERIES_WORDS = ["trilogy", "series", "saga", "chronicles", "sequence", "collection"]

    @staticmethod
    def identify_series(title: str, album: str, file_path: str) -> tuple[str, int, str]:
        """Identify series name and position from title, album, and file path."""
        # Try title first
        series, position, method = SeriesPatternMatcher._check_patterns(title)
        if series:
            return series, position, f"title:{method}"

        # Try album
        if album:
            series, position, method = SeriesPatternMatcher._check_patterns(album)
            if series:
                return series, position, f"album:{method}"

        # Try directory structure
        path_parts = Path(file_path).parts
        if len(path_parts) >= 2:
            parent_dir = path_parts[-2]
            author_dir = path_parts[-3] if len(path_parts) >= 3 else ""

            if parent_dir != author_dir and len(parent_dir.split()) > 1:
                # Check if directory has series pattern
                series, position, method = SeriesPatternMatcher._check_patterns(parent_dir)
                if series:
                    return series, position, f"directory:{method}"

                for word in SeriesPatternMatcher.SERIES_WORDS:
                    if word.lower() in parent_dir.lower():
                        # Clean up directory name
                        clean_dir, pos = SeriesPatternMatcher._clean_series_name(parent_dir)
                        return clean_dir, pos, "directory:keyword"

                if title and (
                    parent_dir.lower() in title.lower() or title.lower() in parent_dir.lower()
                ):
                    # Clean up directory name
                    clean_dir, pos = SeriesPatternMatcher._clean_series_name(parent_dir)
                    return clean_dir, pos, "directory:fuzzy"

        return "", 0, "none"

    @staticmethod
    def _clean_series_name(text: str) -> tuple[str, int]:
        """Clean series name by removing trailing numbers and extracting position."""
        if not text:
            return text, 0

        # Try to extract series number from patterns like "Series Name 03"
        match = re.match(r"^(.+?)\s+(\d{2})$", text)
        if match:
            return match.group(1).strip(), int(match.group(2))

        # Try "Series Name, Book N"
        match = re.match(r"^(.+?),\s+Book\s+(\d+)$", text, re.IGNORECASE)
        if match:
            return match.group(1).strip(), int(match.group(2))

        # Try "Series Name Book N"
        match = re.match(r"^(.+?)\s+Book\s+(\d+)$", text, re.IGNORECASE)
        if match:
            return match.group(1).strip(), int(match.group(2))

        return text, 0

    @staticmethod
    def _check_patterns(text: str) -> tuple[str, int, str]:
        """Check text against all patterns."""
        if not text:
            return "", 0, ""

        for pattern, method in SeriesPatternMatcher.PATTERNS:
            match = pattern.match(text)
            if match:
                groups = match.groups()
                if len(groups) < 2:
                    continue

                series = groups[0].strip()
                position = 0

                # Extract position from second capture group
                try:
                    position = int(groups[1])
                except (ValueError, IndexError):
                    pass

                return series, position, method

        return "", 0, ""


class PathExtractor:
    """Extract metadata from file paths and filenames."""

    SUPPORTED_EXTENSIONS = [".m4b", ".mp3", ".m4a", ".aac", ".flac", ".ogg", ".opus"]

    # Directory names to skip when looking for author
    SKIP_DIRS = {
        "books",
        "audiobooks",
        "bt",
        "incomplete",
        "mnt",
        "bigdata",
        "data",
        "/",
        "newbooks",
        "downloads",
        "media",
        "audio",
        "library",
        "collection",
    }

    @staticmethod
    def extract_author_from_path(file_path: str, fallback_only: bool = True) -> str:
        """Extract author name from directory structure (use as fallback only)."""
        if fallback_only:
            # Only use as fallback - don't extract from common directory names
            return ""

        path_parts = Path(file_path).parts

        meaningful_dirs = []
        for i, part in enumerate(path_parts):
            if part.lower() not in PathExtractor.SKIP_DIRS and i < len(path_parts) - 1:
                meaningful_dirs.append(part)

        if not meaningful_dirs:
            return ""

        first_dir = meaningful_dirs[0]

        # Pattern: "Author, Co-Author - translator - Title-Series, Book N"
        # Example: "Petr Zhgulyov, Sofia Gutkin - translator - City of Goblins-In the System, Book 1"
        if " - translator - " in first_dir or " - narrated by - " in first_dir:
            author_match = re.match(r"^([^-]+)\s*-\s*(?:translator|narrated by)\s*-", first_dir)
            if author_match:
                return author_match.group(1).strip()

        # Check if directory name looks like "Author - Year - Title" format
        if " - " in first_dir and any(char.isdigit() for char in first_dir):
            author_match = re.match(r"^([^-]+)\s*-", first_dir)
            if author_match:
                author = author_match.group(1).strip()
                # Skip invalid author patterns
                if PathExtractor._is_valid_author(author):
                    return author

        return ""

    @staticmethod
    def _is_valid_author(author: str) -> bool:
        """Check if extracted author string is valid (not a book number, chapter, etc.)."""
        if not author:
            return False

        # Skip if it starts with common non-author patterns
        if author.lower().startswith(("book", "chapter", "part", "vol", "volume", "disc")):
            return False

        # Skip if it's purely numeric (like "01", "02", "001")
        if re.match(r"^\d+$", author):
            return False

        # Skip if it's a chapter pattern
        if re.match(r"(?i)^chapter\s+\d+", author):
            return False

        return True

    @staticmethod
    def extract_title_from_filename(filename: str) -> str:
        """Extract title from filename."""
        title = Path(filename).stem

        if re.match(r"(?i)^Chapter\s+\d+", title):
            return ""

        title = re.sub(r"^\d+[-_.\s]+", "", title)
        title = re.sub(r"\s*\((?:Unabridged|Audiobook|Retail)\)$", "", title, flags=re.IGNORECASE)

        return title.strip()

    @staticmethod
    def parse_title_author_from_filename(filename: str) -> tuple[str, str]:
        """
        Parse title and author from filename patterns like:
        - "Title - Author"
        - "Author - Title"
        - "Title_Author" (underscore separator)
        - "Series Name - Author"

        Returns tuple of (title, author).
        """
        stem = Path(filename).stem

        # Remove leading numbers (chapter/track numbers)
        stem = re.sub(r"^\d+[-_.\s]+", "", stem)

        # Remove chapter info from end (e.g., "Title-10 Chapter 10" -> "Title")
        stem = re.sub(r"[-_]\d+\s+Chapter\s+\d+$", "", stem, flags=re.IGNORECASE)

        # Remove common suffixes
        stem = re.sub(r"\s*\((?:Unabridged|Audiobook|Retail)\)$", "", stem, flags=re.IGNORECASE)

        # Try underscore separator first (less common, so check it first)
        if "_" in stem and " - " not in stem:
            parts = stem.split("_", 1)
            if len(parts) == 2:
                left, right = parts[0].strip(), parts[1].strip()
                # Check if this looks like "Title_Author" pattern
                if PathExtractor._looks_like_person_name(
                    right
                ) and not PathExtractor._looks_like_person_name(left):
                    return left, right
                elif PathExtractor._looks_like_person_name(
                    left
                ) and not PathExtractor._looks_like_person_name(right):
                    return right, left

        # Look for " - " separator
        if " - " in stem:
            parts = stem.split(" - ", 1)
            if len(parts) == 2:
                left, right = parts[0].strip(), parts[1].strip()

                # Check if right side looks like a person's name
                right_is_name = PathExtractor._looks_like_person_name(right)
                # Check if left side looks like a person's name
                left_is_name = PathExtractor._looks_like_person_name(left)

                # Decide which is which
                if right_is_name and not left_is_name:
                    # Pattern: "Title - Author"
                    return left, right
                elif left_is_name and not right_is_name:
                    # Pattern: "Author - Title"
                    return right, left
                elif right_is_name:
                    # Both could be names, but prefer "Title - Author" pattern
                    return left, right

        # No clear separator or pattern, return just title
        return stem.strip(), ""

    @staticmethod
    def _looks_like_person_name(s: str) -> bool:
        """Check if a string looks like a person's name."""
        if not s:
            return False

        # Reject invalid author patterns
        if not PathExtractor._is_valid_author(s):
            return False

        # Check for initials like "J. K. Rowling" or "J.K. Rowling"
        if "." in s:
            uppers = sum(1 for c in s if c >= "A" and c <= "Z")
            if uppers >= 2:
                return True

        # Check for multi-word names with proper capitalization
        words = s.split()
        if len(words) >= 2 and len(words) <= 4:
            # Check if all words start with uppercase
            if all(word and word[0].isupper() for word in words):
                return True

        # Check for "FirstName LastName" pattern
        if len(words) >= 2:
            if words[0] and words[0][0].isupper() and words[1] and words[1][0].isupper():
                return True

        return False

    @staticmethod
    def is_chapter_file(filename: str) -> tuple[bool, int | None]:
        """Check if file is a chapter file and extract chapter number."""
        stem = Path(filename).stem
        match = re.match(r"(?i)^Chapter\s+(\d+)", stem)
        if match:
            return True, int(match.group(1))

        match = re.match(r"(?i)^(?:Track\s+)?(\d+)(?:[-_.\s]|$)", stem)
        if match and len(match.group(1)) <= 3:
            return True, int(match.group(1))

        return False, None


class BookGrouper:
    """Groups files into books and identifies duplicates with metadata extraction."""

    def __init__(self, use_metadata: bool = True):
        """Initialize the book grouper."""
        self.use_metadata = use_metadata
        self.books_by_directory: dict[str, list[FileInfo]] = defaultdict(list)
        self.pattern_matcher = SeriesPatternMatcher()
        self.path_extractor = PathExtractor()
        self.metadata_extractor = MetadataExtractor() if use_metadata else None
        self.metadata_cache: dict[str, dict[str, str]] = {}

    def group_files(self, file_paths: list[str]) -> list[BookMetadata]:
        """Group files by book and identify versions/duplicates."""
        print("Phase 1: Grouping files by directory...")
        for i, file_path in enumerate(file_paths):
            if i % 5000 == 0 and i > 0:
                print(f"  Processed {i:,} files...")

            if not self._is_supported_file(file_path):
                continue

            directory = str(Path(file_path).parent)
            filename = Path(file_path).name
            ext = Path(file_path).suffix.lower()

            is_chapter, chapter_num = self.path_extractor.is_chapter_file(filename)

            file_info = FileInfo(
                original_path=file_path,
                filename=filename,
                extension=ext,
                is_chapter=is_chapter,
                chapter_number=chapter_num,
            )

            self.books_by_directory[directory].append(file_info)

        print(f"Phase 2: Extracting metadata from {len(self.books_by_directory):,} directories...")
        books = []
        processed_dirs: set[str] = set()

        for i, (directory, files) in enumerate(self.books_by_directory.items()):
            if i % 100 == 0 and i > 0:
                print(f"  Processed {i:,} directories...")

            if directory in processed_dirs:
                continue

            book = self._create_book_from_directory(directory, files)
            if book:
                books.append(book)
                processed_dirs.add(directory)

        print("Phase 3: Identifying duplicates...")
        self._identify_duplicates(books)

        return books

    def _is_supported_file(self, file_path: str) -> bool:
        """Check if file has supported extension."""
        ext = Path(file_path).suffix.lower()
        return ext in self.path_extractor.SUPPORTED_EXTENSIONS

    def _get_metadata(self, file_path: str) -> dict[str, str]:
        """Get metadata from file (with caching)."""
        if file_path in self.metadata_cache:
            return self.metadata_cache[file_path]

        if self.metadata_extractor and os.path.exists(file_path):
            metadata = self.metadata_extractor.extract_from_file(file_path)
            self.metadata_cache[file_path] = metadata
            return metadata

        return {}

    def _create_book_from_directory(
        self, directory: str, files: list[FileInfo]
    ) -> BookMetadata | None:
        """Create a BookMetadata object from files in a directory."""
        if not files:
            return None

        # Find best file for metadata extraction (non-chapter, or first file)
        primary_file = None
        for f in files:
            if not f.is_chapter:
                primary_file = f
                break
        if not primary_file:
            primary_file = files[0]

        # Extract metadata from file tags
        file_metadata = self._get_metadata(primary_file.original_path)

        # Prioritize metadata from tags
        title = file_metadata.get("title", "").strip()
        author = file_metadata.get("artist", "").strip()
        if not author:
            author = file_metadata.get("album_artist", "").strip()
        album = file_metadata.get("album", "").strip()
        narrator = file_metadata.get("narrator", "").strip()

        metadata_source = "tags" if (title or author) else "path"

        # Fallback to filename/path if no metadata
        if not title or not author:
            # Try to parse title and author from filename
            parsed_title, parsed_author = self.path_extractor.parse_title_author_from_filename(
                primary_file.filename
            )

            if not title and parsed_title:
                title = parsed_title
                metadata_source = "path" if metadata_source == "tags" else "path"

            if not author and parsed_author:
                author = parsed_author
                if metadata_source == "tags":
                    metadata_source = "mixed"

            # Still no author? Try path extraction as last resort
            if not author:
                author = self.path_extractor.extract_author_from_path(
                    primary_file.original_path, fallback_only=False
                )
                if metadata_source == "tags":
                    metadata_source = "mixed"

        # Identify series
        series, position, method = self.pattern_matcher.identify_series(
            title, album, primary_file.original_path
        )

        # Generate book ID
        book_id = self._generate_book_id(title, author, series)

        # Create version
        version_id = hashlib.md5(directory.encode()).hexdigest()[:12]
        version = BookVersion(
            version_id=version_id,
            files=files,
            primary_file=primary_file.original_path,
            format=self._determine_primary_format(files),
            total_files=len(files),
            is_multi_file=len(files) > 1,
            base_directory=directory,
        )

        # Calculate confidence
        confidence = self._calculate_confidence(title, author, series, position, metadata_source)

        # Create book metadata
        book = BookMetadata(
            book_id=book_id,
            title=title or "Unknown_Title",
            author=author or "Unknown_Author",
            series=series,
            position=position,
            narrator=narrator,
            album=album,
            confidence=confidence,
            extraction_method=method,
            proposed_path=self._generate_proposed_path(
                title, author, series, position, version.format
            ),
            versions=[version],
            total_versions=1,
            metadata_source=metadata_source,
        )

        # Identify issues
        self._identify_issues(book)

        return book

    def _generate_book_id(self, title: str, author: str, series: str) -> str:
        """Generate a unique but matchable ID for a book."""
        norm_title = self._normalize_for_comparison(title)
        norm_author = self._normalize_for_comparison(author)
        norm_series = self._normalize_for_comparison(series)

        id_string = f"{norm_author}_{norm_series}_{norm_title}"
        return hashlib.md5(id_string.encode()).hexdigest()[:16]

    def _normalize_for_comparison(self, text: str) -> str:
        """Normalize text for duplicate comparison."""
        if not text:
            return ""
        text = re.sub(r"[^\w\s]", "", text.lower())
        text = re.sub(r"\s+", "_", text.strip())
        return text

    def _determine_primary_format(self, files: list[FileInfo]) -> str:
        """Determine the primary format from a list of files."""
        ext_counts = defaultdict(int)
        for f in files:
            ext_counts[f.extension] += 1

        if ext_counts:
            return max(ext_counts, key=ext_counts.get)
        return ""

    def _calculate_confidence(
        self, title: str, author: str, series: str, position: int, metadata_source: str
    ) -> str:
        """Calculate confidence level for extracted metadata."""
        score = 0

        # Higher weight for metadata from tags
        if metadata_source == "tags":
            score += 2
        elif metadata_source == "mixed":
            score += 1

        if title and title != "Unknown_Title":
            score += 1
        if author and author != "Unknown_Author":
            score += 1
        if series:
            score += 1
        if position > 0:
            score += 1

        if score >= 5:
            return "high"
        elif score >= 3:
            return "medium"
        else:
            return "low"

    def _generate_proposed_path(
        self, title: str, author: str, series: str, position: int, format: str
    ) -> str:
        """Generate proposed organized path structure."""
        parts = []

        if author and author != "Unknown_Author":
            parts.append(self._sanitize_filename(author))
        else:
            parts.append("Unknown_Author")

        if series:
            parts.append(self._sanitize_filename(series))

        filename_parts = []
        if series and position > 0:
            filename_parts.append(f"{position:02d}")
        if title and title != "Unknown_Title":
            filename_parts.append(self._sanitize_filename(title))
        else:
            filename_parts.append("Unknown_Title")

        filename = " - ".join(filename_parts) + format
        parts.append(filename)

        return str(Path(*parts))

    @staticmethod
    def _sanitize_filename(name: str) -> str:
        """Sanitize a string for use in a filename."""
        name = re.sub(r'[<>:"/\\|?*]', "", name)
        name = re.sub(r"\s+", " ", name)
        name = name.strip(". ")
        return name

    def _identify_issues(self, book: BookMetadata) -> None:
        """Identify potential issues with the extracted metadata."""
        if book.title == "Unknown_Title":
            book.issues.append("Missing or unclear title")

        if book.author == "Unknown_Author":
            book.issues.append("Missing or unclear author")

        if book.metadata_source == "path":
            book.issues.append("No embedded metadata found - using path/filename")

        if book.confidence == "low":
            book.issues.append("Low confidence in metadata extraction")

        if book.extraction_method == "none":
            book.issues.append("No series pattern detected")

    def _identify_duplicates(self, books: list[BookMetadata]) -> None:
        """Identify and link duplicate books."""
        books_by_id: dict[str, list[BookMetadata]] = defaultdict(list)

        for book in books:
            books_by_id[book.book_id].append(book)

        duplicate_group_counter = 0
        for book_id, book_group in books_by_id.items():
            if len(book_group) > 1:
                duplicate_group_counter += 1
                group_id = f"DUP{duplicate_group_counter:04d}"

                for book in book_group:
                    book.duplicate_group_id = group_id
                    book.total_versions = len(book_group)


class TestReportGenerator:
    """Generate reports from grouped and analyzed books."""

    def __init__(self):
        """Initialize the report generator."""
        self.stats = defaultdict(int)

    def generate_report(self, books: list[BookMetadata], output_file: str | None = None) -> dict:
        """Generate comprehensive report from scanned books."""
        report = {
            "summary": self._generate_summary(books),
            "statistics": self._generate_statistics(books),
            "books": [self._book_to_dict(book) for book in books],
            "duplicates": self._generate_duplicates_summary(books),
            "issues_summary": self._generate_issues_summary(books),
        }

        if output_file:
            # Ensure output goes to temp_out directory
            output_path = Path(output_file)
            if not output_path.is_absolute() and output_path.parts[0] != "temp_out":
                output_path = Path("temp_out") / output_path.name

            # Create temp_out directory if it doesn't exist
            output_path.parent.mkdir(parents=True, exist_ok=True)

            print(f"\nWriting report to {output_path}...")
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2)

        return report

    def _generate_summary(self, books: list[BookMetadata]) -> dict:
        """Generate high-level summary statistics."""
        total_files = sum(v.total_files for book in books for v in book.versions)
        multi_file_books = sum(1 for book in books for v in book.versions if v.is_multi_file)
        books_with_duplicates = sum(1 for book in books if book.duplicate_group_id)
        metadata_from_tags = sum(1 for book in books if book.metadata_source in ["tags", "mixed"])

        return {
            "total_books": len(books),
            "total_files_processed": total_files,
            "multi_file_books": multi_file_books,
            "books_with_duplicates": books_with_duplicates,
            "unique_duplicate_groups": len(
                {b.duplicate_group_id for b in books if b.duplicate_group_id}
            ),
            "books_with_series": sum(1 for b in books if b.series),
            "books_with_position": sum(1 for b in books if b.position > 0),
            "high_confidence": sum(1 for b in books if b.confidence == "high"),
            "medium_confidence": sum(1 for b in books if b.confidence == "medium"),
            "low_confidence": sum(1 for b in books if b.confidence == "low"),
            "books_with_issues": sum(1 for b in books if b.issues),
            "metadata_from_tags": metadata_from_tags,
            "metadata_from_path_only": len(books) - metadata_from_tags,
        }

    def _generate_statistics(self, books: list[BookMetadata]) -> dict:
        """Generate detailed statistics."""
        formats = defaultdict(int)
        extraction_methods = defaultdict(int)
        authors = defaultdict(int)
        series = defaultdict(int)
        metadata_sources = defaultdict(int)

        for book in books:
            extraction_methods[book.extraction_method] += 1
            metadata_sources[book.metadata_source] += 1
            if book.author and book.author != "Unknown_Author":
                authors[book.author] += 1
            if book.series:
                series[book.series] += 1
            for version in book.versions:
                formats[version.format] += 1

        return {
            "by_format": dict(formats),
            "by_extraction_method": dict(extraction_methods),
            "by_metadata_source": dict(metadata_sources),
            "top_authors": dict(sorted(authors.items(), key=lambda x: x[1], reverse=True)[:30]),
            "top_series": dict(sorted(series.items(), key=lambda x: x[1], reverse=True)[:30]),
        }

    def _generate_duplicates_summary(self, books: list[BookMetadata]) -> list[dict]:
        """Generate summary of duplicate groups."""
        dup_groups: dict[str, list[BookMetadata]] = defaultdict(list)

        for book in books:
            if book.duplicate_group_id:
                dup_groups[book.duplicate_group_id].append(book)

        duplicates = []
        for group_id, group_books in sorted(dup_groups.items()):
            duplicates.append(
                {
                    "group_id": group_id,
                    "title": group_books[0].title,
                    "author": group_books[0].author,
                    "version_count": len(group_books),
                    "versions": [
                        {
                            "version_id": v.version_id,
                            "format": v.format,
                            "file_count": v.total_files,
                            "directory": v.base_directory,
                        }
                        for book in group_books
                        for v in book.versions
                    ],
                }
            )

        return duplicates

    def _generate_issues_summary(self, books: list[BookMetadata]) -> dict:
        """Generate summary of common issues."""
        issue_counts = defaultdict(int)

        for book in books:
            for issue in book.issues:
                issue_counts[issue] += 1

        return dict(sorted(issue_counts.items(), key=lambda x: x[1], reverse=True))

    def _book_to_dict(self, book: BookMetadata) -> dict:
        """Convert BookMetadata to dictionary."""
        return {
            "book_id": book.book_id,
            "title": book.title,
            "author": book.author,
            "series": book.series,
            "position": book.position,
            "narrator": book.narrator,
            "album": book.album,
            "confidence": book.confidence,
            "extraction_method": book.extraction_method,
            "metadata_source": book.metadata_source,
            "proposed_path": book.proposed_path,
            "issues": book.issues,
            "duplicate_group_id": book.duplicate_group_id,
            "total_versions": book.total_versions,
            "versions": [
                {
                    "version_id": v.version_id,
                    "format": v.format,
                    "total_files": v.total_files,
                    "is_multi_file": v.is_multi_file,
                    "base_directory": v.base_directory,
                    "files": [
                        {
                            "path": f.original_path,
                            "filename": f.filename,
                            "extension": f.extension,
                            "is_chapter": f.is_chapter,
                            "chapter_number": f.chapter_number,
                        }
                        for f in v.files[:10]  # Limit to first 10 files in JSON
                    ]
                    + (
                        [{"note": f"... and {len(v.files) - 10} more files"}]
                        if len(v.files) > 10
                        else []
                    ),
                }
                for v in book.versions
            ],
        }

    def print_summary(self, report: dict) -> None:
        """Print a human-readable summary."""
        summary = report["summary"]
        stats = report["statistics"]

        print("\n" + "=" * 80)
        print("AUDIOBOOK ORGANIZER TEST REPORT V3 (with Metadata Extraction)")
        print("=" * 80)

        print("\nSUMMARY:")
        print(f"  Total books identified: {summary['total_books']:,}")
        print(f"  Total files processed: {summary['total_files_processed']:,}")
        print(f"  Multi-file books: {summary['multi_file_books']:,}")
        print(f"  Books with duplicate versions: {summary['books_with_duplicates']:,}")
        print(f"  Unique duplicate groups: {summary['unique_duplicate_groups']:,}")

        print("\nMETADATA SOURCES:")
        print(f"  From embedded tags: {summary['metadata_from_tags']:,}")
        print(f"  From path/filename only: {summary['metadata_from_path_only']:,}")

        print("\nSERIES DETECTION:")
        print(f"  Books with series identified: {summary['books_with_series']:,}")
        print(f"  Books with series position: {summary['books_with_position']:,}")

        print("\nCONFIDENCE LEVELS:")
        print(f"  High confidence: {summary['high_confidence']:,}")
        print(f"  Medium confidence: {summary['medium_confidence']:,}")
        print(f"  Low confidence: {summary['low_confidence']:,}")
        print(f"\n  Books with issues: {summary['books_with_issues']:,}")

        print("\nFILE FORMATS:")
        for format, count in sorted(stats["by_format"].items()):
            print(f"  {format}: {count:,}")

        print("\nMETADATA SOURCES:")
        for source, count in sorted(stats["by_metadata_source"].items()):
            print(f"  {source}: {count:,}")

        print("\nEXTRACTION METHODS:")
        for method, count in sorted(
            stats["by_extraction_method"].items(), key=lambda x: x[1], reverse=True
        ):
            print(f"  {method}: {count:,}")

        print("\nTOP 15 AUTHORS:")
        for author, count in list(stats["top_authors"].items())[:15]:
            print(f"  {author}: {count:,} books")

        print("\nTOP 15 SERIES:")
        for series, count in list(stats["top_series"].items())[:15]:
            print(f"  {series}: {count:,} books")

        if "duplicates" in report and report["duplicates"]:
            print(f"\nDUPLICATE GROUPS (showing first 5 of {len(report['duplicates'])}):")
            for dup in report["duplicates"][:5]:
                print(f"  {dup['group_id']}: {dup['author']} - {dup['title']}")
                print(f"    Versions: {dup['version_count']}")
                for v in dup["versions"][:3]:
                    print(f"      - {v['format']} ({v['file_count']} files)")

        if "issues_summary" in report and report["issues_summary"]:
            print("\nCOMMON ISSUES:")
            for issue, count in list(report["issues_summary"].items())[:5]:
                print(f"  {issue}: {count:,}")

        print("\n" + "=" * 80)


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Test audiobook organizer V3 - with metadata extraction from file tags"
    )
    parser.add_argument("file_list", help="Path to file list")
    parser.add_argument(
        "--output",
        "-o",
        help="Output JSON file (will be placed in temp_out/)",
        default="organize-test-report-v3.json",
    )
    parser.add_argument("--limit", "-l", type=int, help="Limit number of files")
    parser.add_argument("--sample", "-s", type=int, help="Sample books to display", default=5)
    parser.add_argument(
        "--no-metadata", action="store_true", help="Skip metadata extraction (faster)"
    )

    args = parser.parse_args()

    if not os.path.exists(args.file_list):
        print(f"Error: File not found: {args.file_list}", file=sys.stderr)
        sys.exit(1)

    print(f"Reading file list from: {args.file_list}")

    with open(args.file_list, encoding="utf-8") as f:
        file_paths = [line.strip() for line in f if line.strip()]

    if args.limit:
        file_paths = file_paths[: args.limit]

    print(f"Processing {len(file_paths):,} files...")
    if not args.no_metadata:
        print("Metadata extraction enabled (using ffprobe)")
    else:
        print("Metadata extraction disabled - using path/filename only")

    # Group files into books
    grouper = BookGrouper(use_metadata=not args.no_metadata)
    books = grouper.group_files(file_paths)

    print(f"\nIdentified {len(books):,} unique books")

    # Generate report
    report_gen = TestReportGenerator()
    report = report_gen.generate_report(books, args.output)

    # Print summary
    report_gen.print_summary(report)

    # Print sample books
    if args.sample and books:
        print(f"\nSAMPLE OF {min(args.sample, len(books))} BOOKS:")
        print("-" * 80)
        for book in books[: args.sample]:
            print(f"\n[{book.book_id}] {book.author} - {book.title}")
            if book.series:
                print(f"  Series: {book.series} #{book.position if book.position else 'N/A'}")
            if book.narrator:
                print(f"  Narrator: {book.narrator}")
            print(
                f"  Source: {book.metadata_source}, Method: {book.extraction_method}, Confidence: {book.confidence}"
            )
            print(f"  Proposed: {book.proposed_path}")
            if book.duplicate_group_id:
                print(
                    f"  ⚠️  DUPLICATE: Group {book.duplicate_group_id} ({book.total_versions} versions)"
                )
            print(f"  Versions: {len(book.versions)}")
            for i, version in enumerate(book.versions, 1):
                print(f"    Version {i}: {version.format} - {version.total_files} file(s)")
                if version.is_multi_file:
                    for file in version.files[:2]:
                        chapter_info = (
                            f" (Chapter {file.chapter_number})" if file.is_chapter else ""
                        )
                        print(f"      - {file.filename}{chapter_info}")
                    if len(version.files) > 2:
                        print(f"      ... and {len(version.files) - 2} more files")
            if book.issues:
                print(f"  Issues: {', '.join(book.issues)}")

    print(f"\nDetailed report written to: {args.output}")


if __name__ == "__main__":
    main()
